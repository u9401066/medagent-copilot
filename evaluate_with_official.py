#!/usr/bin/env python3
"""
ä½¿ç”¨å®˜æ–¹ MedAgentBench è©•ä¼°å™¨

é€™å€‹è…³æœ¬ç›´æ¥ä½¿ç”¨å®˜æ–¹çš„ eval.py é€²è¡Œè©•ä¼°
**ä¸åšä»»ä½•è³‡æ–™æ ¼å¼è½‰æ›** - MCP è¼¸å‡ºçš„æ ¼å¼å¿…é ˆèˆ‡å®˜æ–¹å®Œå…¨ä¸€è‡´
"""

import json
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ  MedAgentBench åˆ°è·¯å¾‘
MEDAGENTBENCH_PATH = Path("/home/eric/workspace251126/MedAgentBench")
sys.path.insert(0, str(MEDAGENTBENCH_PATH))
sys.path.insert(0, str(MEDAGENTBENCH_PATH / "src"))

FHIR_BASE = "http://localhost:8080/fhir/"
RESULTS_PATH = Path("/home/eric/workspace251126/medagent-copilot/results")

# ä½¿ç”¨å®˜æ–¹é¡å‹ - ä¸è‡ªå®šç¾©ä»»ä½•æ ¼å¼
from src.typings.general import ChatHistoryItem
from src.typings.output import TaskOutput


def build_official_result(result_entry: dict) -> TaskOutput:
    """ç›´æ¥ä½¿ç”¨å®˜æ–¹é¡å‹å»ºæ§‹ TaskOutput
    
    æ³¨æ„ï¼šé€™è£¡ä¸åšä»»ä½•æ ¼å¼è½‰æ›ï¼
    MCP è¼¸å‡ºçš„ post_history å¿…é ˆå·²ç¶“æ˜¯å®˜æ–¹æ ¼å¼ï¼š
    - role: "user" æˆ– "agent"
    - content: str (POST æ ¼å¼: "POST {url}\n{json}")
    """
    return TaskOutput(
        result=result_entry["answer"],
        history=[
            ChatHistoryItem(role=h["role"], content=h["content"])
            for h in result_entry.get("post_history", [])
        ]
    )


def main():
    import argparse
    parser = argparse.ArgumentParser(description='Evaluate MedAgentBench results')
    parser.add_argument('--version', '-v', type=str, default=None, 
                        help='Version to evaluate (v1 or v2). Auto-detect if not specified.')
    parser.add_argument('--file', '-f', type=str, default=None,
                        help='Specific results file to evaluate')
    args = parser.parse_args()
    
    # å°å…¥å®˜æ–¹è©•ä¼°å™¨
    from src.server.tasks.medagentbench.eval import eval as official_eval
    
    # è¼‰å…¥çµæœæª”æ¡ˆ
    if args.file:
        results_file = Path(args.file)
    else:
        # æ ¹æ“šç‰ˆæœ¬æ‰¾æª”æ¡ˆ
        if args.version:
            pattern = f"results_{args.version}_*.json"
        else:
            pattern = "results_*.json"
        
        results_files = list(RESULTS_PATH.glob(pattern))
        if not results_files:
            print(f"No results file found matching {pattern}")
            return
        
        results_file = max(results_files, key=lambda p: p.stat().st_mtime)
    
    print(f"ğŸ“ Evaluating: {results_file}")
    
    with open(results_file) as f:
        data = json.load(f)
    
    results_list = data["results"]
    version = data.get("version", "v1")
    
    # è¼‰å…¥ä»»å‹™è³‡æ–™ - æ ¹æ“šç‰ˆæœ¬é¸æ“‡æ­£ç¢ºçš„æ¸¬è©¦æª”æ¡ˆ
    task_file = MEDAGENTBENCH_PATH / "data" / "medagentbench" / f"test_data_{version}.json"
    with open(task_file) as f:
        all_tasks = json.load(f)
    task_dict = {t["id"]: t for t in all_tasks}
    
    # è©•ä¼°
    stats = {}
    details = []
    
    print("\n" + "=" * 70)
    print("ğŸ“Š OFFICIAL EVALUATION (using MedAgentBench eval.py)")
    print("=" * 70)
    
    for r in results_list:
        task_id = r["task_id"]
        task_type = task_id.split("_")[0]
        
        if task_type not in stats:
            stats[task_type] = {"correct": 0, "total": 0}
        stats[task_type]["total"] += 1
        
        # å»ºç«‹å®˜æ–¹æ ¼å¼
        case_data = task_dict.get(task_id, {}).copy()
        case_data["eval_MRN"] = r.get("eval_MRN")
        case_data["id"] = task_id
        
        official_result = build_official_result(r)
        
        # èª¿ç”¨å®˜æ–¹è©•ä¼°
        try:
            is_correct = official_eval(case_data, official_result, FHIR_BASE)
            if is_correct is None:
                is_correct = False
        except Exception as e:
            print(f"  Error in {task_id}: {e}")
            is_correct = False
        
        if is_correct:
            stats[task_type]["correct"] += 1
        
        details.append({
            "task_id": task_id,
            "correct": is_correct,
            "answer": r["answer"],
            "post_count": r.get("post_count", 0)
        })
    
    # è¼¸å‡ºçµæœ
    print()
    total_correct = 0
    total_count = 0
    
    for task_type in sorted(stats.keys()):
        s = stats[task_type]
        pct = s["correct"] / s["total"] * 100 if s["total"] > 0 else 0
        total_correct += s["correct"]
        total_count += s["total"]
        status = "âœ…" if pct == 100 else "âš ï¸" if pct >= 50 else "âŒ"
        print(f"{status} {task_type}: {s['correct']}/{s['total']} ({pct:.0f}%)")
    
    print("-" * 70)
    print(f"ğŸ¯ TOTAL: {total_correct}/{total_count} ({total_correct/total_count*100:.1f}%)")
    print("=" * 70)
    
    # é¡¯ç¤ºéŒ¯èª¤
    incorrect = [d for d in details if not d["correct"]]
    if incorrect:
        print(f"\nğŸ” INCORRECT ({len(incorrect)} items):")
        for d in incorrect[:20]:  # åªé¡¯ç¤ºå‰ 20 å€‹
            print(f"  {d['task_id']}: posts={d['post_count']}, answer={str(d['answer'])[:50]}")
        if len(incorrect) > 20:
            print(f"  ... and {len(incorrect) - 20} more")
    
    # ä¿å­˜åˆ°ä¾†æºæª”æ¡ˆçš„åŒä¸€å€‹è³‡æ–™å¤¾
    eval_output = results_file.parent / "evaluation.json"
    with open(eval_output, "w") as f:
        json.dump({
            "evaluated_at": datetime.now().isoformat(),
            "source_file": results_file.name,
            "version": version,
            "stats": stats,
            "total_correct": total_correct,
            "total_count": total_count,
            "accuracy": f"{total_correct/total_count*100:.1f}%",
            "details": details
        }, f, indent=2)
    print(f"\nğŸ“ Saved to: {eval_output}")


if __name__ == "__main__":
    main()
