#!/usr/bin/env python3
"""
ä½¿ç”¨å®˜æ–¹ MedAgentBench è©•ä¼°å™¨

é€™å€‹è…³æœ¬ï¼š
1. è®€å–æˆ‘å€‘çš„çµæœæª”æ¡ˆ
2. å°‡å…¶è½‰æ›ç‚ºå®˜æ–¹ TaskOutput æ ¼å¼
3. ç›´æ¥èª¿ç”¨å®˜æ–¹ eval.py é€²è¡Œè©•ä¼°
"""

import json
import sys
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Literal

# æ·»åŠ  MedAgentBench åˆ°è·¯å¾‘
MEDAGENTBENCH_PATH = Path("/home/eric/workspace251126/MedAgentBench")
sys.path.insert(0, str(MEDAGENTBENCH_PATH))
sys.path.insert(0, str(MEDAGENTBENCH_PATH / "src"))

FHIR_BASE = "http://localhost:8080/fhir/"
RESULTS_PATH = Path("/home/eric/workspace251126/medagent-copilot/results")


# ============ æ¨¡æ“¬å®˜æ–¹é¡å‹ ============

@dataclass
class ChatHistoryItem:
    """å®˜æ–¹ ChatHistoryItem æ ¼å¼"""
    role: str  # "user" æˆ– "agent"
    content: str


@dataclass
class TaskOutput:
    """å®˜æ–¹ TaskOutput æ ¼å¼"""
    result: str = None  # FINISH ä¸­çš„å…§å®¹
    history: List[ChatHistoryItem] = field(default_factory=list)
    status: str = "COMPLETED"
    index: int = None


def build_official_result(result_entry: dict) -> TaskOutput:
    """å°‡æˆ‘å€‘çš„çµæœè½‰æ›ç‚ºå®˜æ–¹æ ¼å¼"""
    answer = result_entry["answer"]
    post_history = result_entry.get("post_history", [])
    
    # å»ºç«‹ history
    history = []
    for entry in post_history:
        history.append(ChatHistoryItem(
            role=entry["role"],
            content=entry["content"]
        ))
    
    return TaskOutput(
        result=answer,
        history=history
    )


def main():
    # å°å…¥å®˜æ–¹è©•ä¼°å™¨
    from src.server.tasks.medagentbench.eval import eval as official_eval
    
    # è¼‰å…¥çµæœæª”æ¡ˆ
    results_files = list(RESULTS_PATH.glob("results_v1_*.json"))
    if not results_files:
        print("No results file found")
        return
    
    results_file = max(results_files, key=lambda p: p.stat().st_mtime)
    print(f"ğŸ“ Evaluating: {results_file}")
    
    with open(results_file) as f:
        data = json.load(f)
    
    results_list = data["results"]
    
    # è¼‰å…¥ä»»å‹™è³‡æ–™
    task_file = MEDAGENTBENCH_PATH / "data" / "medagentbench" / "test_data_v1.json"
    with open(task_file) as f:
        all_tasks = json.load(f)
    task_dict = {t["id"]: t for t in all_tasks}
    
    # è©•ä¼°
    stats = {}
    details = []
    
    print("\n" + "=" * 70)
    print("ğŸ“Š OFFICIAL EVALUATION (using MedAgentBench eval.py)")
    print("=" * 70)
    
    for r in results_list:
        task_id = r["task_id"]
        task_type = task_id.split("_")[0]
        
        if task_type not in stats:
            stats[task_type] = {"correct": 0, "total": 0}
        stats[task_type]["total"] += 1
        
        # å»ºç«‹å®˜æ–¹æ ¼å¼
        case_data = task_dict.get(task_id, {}).copy()
        case_data["eval_MRN"] = r.get("eval_MRN")
        case_data["id"] = task_id
        
        official_result = build_official_result(r)
        
        # èª¿ç”¨å®˜æ–¹è©•ä¼°
        try:
            is_correct = official_eval(case_data, official_result, FHIR_BASE)
            if is_correct is None:
                is_correct = False
        except Exception as e:
            print(f"  Error in {task_id}: {e}")
            is_correct = False
        
        if is_correct:
            stats[task_type]["correct"] += 1
        
        details.append({
            "task_id": task_id,
            "correct": is_correct,
            "answer": r["answer"],
            "post_count": r.get("post_count", 0)
        })
    
    # è¼¸å‡ºçµæœ
    print()
    total_correct = 0
    total_count = 0
    
    for task_type in sorted(stats.keys()):
        s = stats[task_type]
        pct = s["correct"] / s["total"] * 100 if s["total"] > 0 else 0
        total_correct += s["correct"]
        total_count += s["total"]
        status = "âœ…" if pct == 100 else "âš ï¸" if pct >= 50 else "âŒ"
        print(f"{status} {task_type}: {s['correct']}/{s['total']} ({pct:.0f}%)")
    
    print("-" * 70)
    print(f"ğŸ¯ TOTAL: {total_correct}/{total_count} ({total_correct/total_count*100:.1f}%)")
    print("=" * 70)
    
    # é¡¯ç¤ºéŒ¯èª¤
    incorrect = [d for d in details if not d["correct"]]
    if incorrect:
        print(f"\nğŸ” INCORRECT ({len(incorrect)} items):")
        for d in incorrect[:20]:  # åªé¡¯ç¤ºå‰ 20 å€‹
            print(f"  {d['task_id']}: posts={d['post_count']}, answer={str(d['answer'])[:50]}")
        if len(incorrect) > 20:
            print(f"  ... and {len(incorrect) - 20} more")
    
    # ä¿å­˜
    eval_output = RESULTS_PATH / f"official_eval_{results_file.stem}.json"
    with open(eval_output, "w") as f:
        json.dump({
            "evaluated_at": datetime.now().isoformat(),
            "source_file": str(results_file),
            "stats": stats,
            "total_correct": total_correct,
            "total_count": total_count,
            "accuracy": f"{total_correct/total_count*100:.1f}%",
            "details": details
        }, f, indent=2)
    print(f"\nğŸ“ Saved to: {eval_output}")


if __name__ == "__main__":
    main()
